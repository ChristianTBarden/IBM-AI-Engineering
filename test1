from langchain.retrievers import ParentDocumentRetriever
from langchain_text_splitters import CharacterTextSplitter
from langchain.storage import InMemoryStore

Copied!

Wrap Toggled!
Multi-Query Retriever	The Multi Query Retriever uses an LLM to generate multiple queries from different perspectives for a given user input query. For each query, it retrieves a set of relevant documents and then takes the unique union of these results to form a larger set of potentially relevant documents.
1
2
3
4
def text_to_emb(list_of_text,max_input=512):
    data_token_index  = tokenizer.batch_encode_plus(list_of_text, add_special_tokens=True,padding=True,truncation=True,max_length=max_input)
    question_embeddings=aggregate_embeddings(data_token_index['input_ids'], data_token_index['attention_mask'])
    return question_embeddings

Copied!

Wrap Toggled!
sum calculator	An application that can calculate the sum of your input numbers in Gradio.
1
2
3
4
5
6
7
8
9
10
11
12
13
import gradio as gr

def add_numbers(Num1, Num2):
    return Num1 + Num2

# Define the interface
demo
=
 gr
.
Interface
(
    fn
=
add_numbers
,

    inputs
=[
gr
.
Number
(),
 gr
.
Number
()],

# Create two numerical input fields where users can enter numbers
    outputs
=
gr
.
Number
()

# Create numerical output fields
)


# Launch the interface
demo
.
launch
(
server_name
=
"127.0.0.1"
,
 server_port
=

7860
)

Copied!

Wrap Toggled!
Integrate application into Gradio	You can integrate an application with Gradio to leverage a web interface for inputting questions and receiving responses.
This code guides you through this integration process. It includes three components:

Initializing the model
Defining the function that generates responses from the LLM
Constructing the Gradio interface, enabling interaction with the LLM
1
2
3
4
5
6
# Import necessary packages
from ibm_watsonx_ai.foundation_models import ModelInference
from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as 		GenParams
from ibm_watsonx_ai import Credentials
from langchain_ibm import WatsonxLLM
import gradio as gr

Copied!

Wrap Toggled!
1
2
3
# Model and project settings
model_id = 'mistralai/mixtral-8x7b-instruct-v01' # Directly specifying the model


Copied!

Wrap Toggled!
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
# Set necessary parameters
parameters = {
    GenParams.MAX_NEW_TOKENS: 256,  # Specifying the max tokens you want to generate
    GenParams.TEMPERATURE: 0.5, # This randomness or creativity of the model's responses
}

project_id = “skills-network”

# Wrap up the model into WatsonxLLM inference
watsonx_llm
=

WatsonxLLM
(
    model_id
=
model_id
,
    url
=
"https://us-south.ml.cloud.ibm.com"
,
    project_id
=
project_id
,

params
=
parameters
,
)

# Function to generate a response from the model
def
 generate_response
(
prompt_txt
):
    generated_response
=
 watsonx_llm
.
invoke
(
prompt_txt
)

return
 generated_response

# Create Gradio interface
chat_application
=
 gr
.
Interface
(
    fn
=
generate_response
,
    allow_flagging
=
"never"
,
    inputs
=
gr
.
Textbox
(
label
=
"Input"
,
 lines
=
2
,
 placeholder
=
"Type your question here..."
),
    outputs
=
gr
.
Textbox
(
label
=
"Output"
),
    title
=
"Watsonx.ai Chatbot"
,
description
=
"Ask any question and the chatbot will try to answer."
)

# Launch the app
chat_application
.
launch
(
server_name
=
"127.0.0.1"
,
 server_port
=

7860
)

</td>


Copied!

Wrap Toggled!
Initialize the LLM	You can initialize the LLM by creating an instance of WatsonxLLM, a class in langchain_ibm. WatsonxLLM can use several underlying foundational models. In this snippet, you use Mixtral 8x7B.
To initialize the LLM, paste the following code into qabot.py. Note that you are initializing the model with a temperature of 0.5, and allowing for the generation of a maximum of 256 tokens.

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
## LLM
def get_llm():
    model_id = 'mistralai/mixtral-8x7b-instruct-v01'
    parameters = {
        GenParams.MAX_NEW_TOKENS: 256,
        GenParams.TEMPERATURE: 0.5,
    }
    project_id = "skills-network"
    watsonx_llm = WatsonxLLM(
        model_id=model_id,
        url="https://us-south.ml.cloud.ibm.com",
        project_id=project_id,
        params=parameters,
    )
    return watsonx_llm

Copied!

Wrap Toggled!
Define the PDF document loader	You use the PyPDFLoader class from the langchain_community library to load PDF documents.
You create the PDF loader as an instance of PyPDFLoader. Then, you load the document and return the loaded document. To incorporate the PDF loader in your bot, add the following code to qabot.py.

1
2
3
4
5
## Document loader
def document_loader(file):
    loader = PyPDFLoader(file.name)
    loaded_document = loader.load()
    return loaded_document

Copied!

Wrap Toggled!
Define the text splitter	You define a document splitter that will split the text into chunks. Add the following code to aqbot.py to define such a text splitter. Note that, in this example, you are defining a RecursiveCharacterTextSplitter with a chunk size of 1000, although other splitters or parameter values are possible.
1
2
3
4
5
6
7
8
9
## Text splitter
def text_splitter(data):
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=50,
        length_function=len,
    )
    chunks = text_splitter.split_documents(data)
    return chunks

Copied!

Wrap Toggled!
Define the vector store	Add this code to qabot.py to define a function that embeds the chunks using a yet-to-be-defined embedding model and stores the embeddings in a ChromaDB vector store.
1
2
3
4
5
## Vector db
def vector_database(chunks):
    embedding_model = watsonx_embedding()
    vectordb = Chroma.from_documents(chunks, embedding_model)
    return vectordb

Copied!

Wrap Toggled!
Define the embedding model	Defines a watsonx_embedding() function that returns an instance of WatsonxEmbeddings, a class from langchain_ibm that generates embeddings. In this case, the embeddings are generated using IBM's Slate 125M English embeddings model. Paste this code into the qabot.py file.
1
2
3
4
5
6
7
8
9
10
11
12
13
## Embedding model
def watsonx_embedding():
    embed_params = {
        EmbedTextParamsMetaNames.TRUNCATE_INPUT_TOKENS: 3,
        EmbedTextParamsMetaNames.RETURN_OPTIONS: {"input_text": True},
    }
    watsonx_embedding = WatsonxEmbeddings(
        model_id="ibm/slate-125m-english-rtrvr",
        url="https://us-south.ml.cloud.ibm.com",
        project_id="skills-network",
        params=embed_params,
    )
    return watsonx_embedding

Copied!

Wrap Toggled!
Define a question-answering chain	Use RetrievalQA from LangChain, a chain that performs natural-language question-answering over a data source using retrieval-augmented generation (RAG). Add the following code to qabot.py to define a question-answering chain.
1
2
3
4
5
6
7
8
9
10
## QA Chain
def retriever_qa(file, query):
    llm = get_llm()
    retriever_obj = retriever(file)
    qa = RetrievalQA.from_chain_type(llm=llm,
                                    chain_type="stuff",
                                    retriever=retriever_obj,
                                    return_source_documents=False)
    response = qa.invoke(query)
    return response['result']

Copied!

Wrap Toggled!
Setup the Gradio interface	A Gradio interface should include:
A file upload functionality (provided by the File class in Gradio)
An input textbox where the question can be asked (provided by the Textbox class in Gradio)
An output textbox where the question can be answered (provided by the Textbox class in Gradio)
Add the following code to qabot.py to add the Gradio interface.

1
2
3
4
5
6
7
8
9
10
11
12
# Create Gradio interface
rag_application = gr.Interface(
    fn=retriever_qa,
    allow_flagging="never",
    inputs=[
        gr.File(label="Upload PDF File", file_count="single", file_types=['.pdf'], type="filepath"),  # Drag and drop file upload
        gr.Textbox(label="Input Query", lines=2, placeholder="Type your question here...")
    ],
    outputs=gr.Textbox(label="Output"),
    title="RAG Chatbot",
    description="Upload a PDF document and ask any question. The chatbot will try to answer using the provided document."
)

Copied!

Wrap Toggled!
Add code to launch the application	Add this line to qabot.py to launch the application using port 7860.
1
2
# Launch the app
rag_application.launch(server_name="0.0.0.0", server_port= 7860)

Copied!

Wrap Toggled!
Verify	The qabot.py should look like this.
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
from ibm_watsonx_ai.foundation_models import ModelInference
from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams
from ibm_watsonx_ai.metanames import EmbedTextParamsMetaNames
from ibm_watsonx_ai import Credentials
from langchain_ibm import WatsonxLLM, WatsonxEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_community.document_loaders import PyPDFLoader
from langchain.chains import RetrievalQA

import gradio as gr

# You can use this section to suppress warnings generated by your code:
def
 warn
(*
args
,

**
kwargs
):

pass
import
 warnings
warnings
.
warn
=
 warn
warnings
.
filterwarnings
(
'ignore'
)

## LLM
def
 get_llm
():
    model_id
=

'mistralai/mixtral-8x7b-instruct-v01'
    parameters
=

{

GenParams
.
MAX_NEW_TOKENS
:

256
,

GenParams
.
TEMPERATURE
:

0.5
,

}
    project_id
=

"skills-network"
    watsonx_llm
=

WatsonxLLM
(
        model_id
=
model_id
,
        url
=
"https://us-south.ml.cloud.ibm.com"
,
        project_id
=
project_id
,

params
=
parameters
,

)

return
 watsonx_llm

## Document loader
def
 document_loader
(
file
):
    loader
=

PyPDFLoader
(
file
.
name
)
    loaded_document
=
 loader
.
load
()

return
 loaded_document

## Text splitter
def
 text_splitter
(
data
):
    text_splitter
=

RecursiveCharacterTextSplitter
(
        chunk_size
=
1000
,
        chunk_overlap
=
50
,
        length_function
=
len
,

)
    chunks
=
 text_splitter
.
split_documents
(
data
)

return
 chunks

## Vector db
def
 vector_database
(
chunks
):
    embedding_model
=
 watsonx_embedding
()
    vectordb
=

Chroma
.
from_documents
(
chunks
,
 embedding_model
)

return
 vectordb

## Embedding model
def
 watsonx_embedding
():
    embed_params
=

{

EmbedTextParamsMetaNames
.
TRUNCATE_INPUT_TOKENS
:

3
,

EmbedTextParamsMetaNames
.
RETURN_OPTIONS
:

{
"input_text"
:

True
},

}
    watsonx_embedding
=

WatsonxEmbeddings
(
        model_id
=
"ibm/slate-125m-english-rtrvr"
,
        url
=
"https://us-south.ml.cloud.ibm.com"
,
        project_id
=
"skills-network"
,

params
=
embed_params
,

)

return
 watsonx_embedding

## Retriever
def
 retriever
(
file
):
    splits
=
 document_loader
(
file
)
    chunks
=
 text_splitter
(
splits
)
    vectordb
=
 vector_database
(
chunks
)
    retriever
=
 vectordb
.
as_retriever
()

return
 retriever

## QA Chain
def
 retriever_qa
(
file
,
 query
):
    llm
=
 get_llm
()
    retriever_obj
=
 retriever
(
file
)
    qa
=

RetrievalQA
.
from_chain_type
(
llm
=
llm
,

                                    chain_type
=
"stuff"
,

                                    retriever
=
retriever_obj
,

                                    return_source_documents
=
False
)
    response
=
 qa
.
invoke
(
query
)

return
 response
[
'result'
]

# Create Gradio interface
rag_application
=
 gr
.
Interface
(
    fn
=
retriever_qa
,
    allow_flagging
=
"never"
,
    inputs
=[
        gr
.
File
(
label
=
"Upload PDF File"
,
 file_count
=
"single"
,
 file_types
=[
'.pdf'
],
 type
=
"filepath"
),

# Drag and drop file upload
        gr
.
Textbox
(
label
=
"Input Query"
,
 lines
=
2
,
 placeholder
=
"Type your question here..."
)

],
    outputs
=
gr
.
Textbox
(
label
=
"Output"
),
    title
=
"RAG Chatbot"
,
    description
=
"Upload a PDF document and ask any question. The chatbot will try to answer using the provided document."
)

# Launch the app
rag_application
.
launch
(
server_name
=
"0.0.0.0"
,
 server_port
=

7860
)

Copied!

Wrap Toggled!
Serve the application	To serve the application, paste this code into your Python terminal:
1
python3.11 qabot.py